{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trabajo práctico 2\n",
        "### Alumnos: Francisco Frusto Alvarado, Ezequiel Kaplan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yc_QGcPrvhgj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wpp8zX0evhgn"
      },
      "outputs": [],
      "source": [
        "#Inicializacion de pesos\n",
        "W1 = np.random.random((5,6))\n",
        "b1 = np.random.random((5,1))\n",
        "\n",
        "W2 = np.random.random((1,5))\n",
        "b2 = np.random.random((1,1))\n",
        "\n",
        "theta = (W1,b1,W2,b2)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para implementar la funcion forward, vamos a hacer los cálculos de a pasos, sabiendo:\n",
        "\n",
        "$\\newline f_{\\theta}(\\mathrm{\\mathbf{x}}) = W_{2} \\ \\sigma (W_{1} \\mathrm{\\mathbf{x}} + b_{1}) + b{2} $\n",
        "\n",
        "En una primera instancia calculamos $ z_{1} = W_{1} \\mathrm{\\mathbf{x}} + b_{1}$ en la que hacemos el producto punto entre $\\mathrm{\\mathbf{x}}$ y $W_{1}$ y luego le sumamos el vector $b_{1}$. \n",
        "\n",
        "En un segundo paso, habiendo creado la funcion sigmoid que calcula: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ vamos a aplicarle esta funcion a los elementos de $z_{1}$\n",
        "\n",
        "En un tercer paso, calculamos el resultado final en el que se hace el producto punto entre $W_{2}$ y el resultado anterior y ademas, se le suma el vector $b_{2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward(theta, x):\n",
        "    W1, b1, W2, b2 = theta\n",
        "\n",
        "    z1 = np.dot(W1, x) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(W2, a1) + b2\n",
        "\n",
        "    return z2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La funcion obejtivo que buscamos minimizar, sabemos que está dada por: \n",
        "$\\newline L = \\frac{1}{2}(f_{\\theta_{t}}(\\mathrm{\\mathbf{x}}_{i}) - y_{i})^2$\n",
        "\n",
        "Para calcular el gradiente de forma numércia, vamos a usar la estrategia propuesta por la cátedra para calcular las derivadas parciales. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-R0FjEzvhgn"
      },
      "outputs": [],
      "source": [
        "#Calculo del gradiente numerico\n",
        "\n",
        "def funcion_objetivo(theta, x, y):\n",
        "    loss = 0.5 * (forward(theta, x) - y)**2\n",
        "    return loss\n",
        "\n",
        "def numerical_gradient(theta, x, y, eps):\n",
        "    epsilon = eps  # Pequeña cantidad para el cálculo numérico del gradiente\n",
        "    gradiente = np.zeros_like(theta)\n",
        "\n",
        "    for i in range(len(theta)):\n",
        "        theta_plus = theta.copy()\n",
        "        theta_minus = theta.copy()\n",
        "\n",
        "        # Aumentar y disminuir un poco el parámetro actual para calcular el gradiente\n",
        "        theta_plus[i] += epsilon\n",
        "        theta_minus[i] -= epsilon\n",
        "\n",
        "        # Calcular las pérdidas para los parámetros aumentados y disminuidos\n",
        "        loss_plus = funcion_objetivo(theta_plus, x, y)\n",
        "        loss_minus = funcion_objetivo(theta_minus, x, y)\n",
        "\n",
        "        # Calcular el gradiente parcial utilizando las derivadas parciales\n",
        "        gradiente[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
        "\n",
        "    return gradiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJRtIYxMvhgo"
      },
      "outputs": [],
      "source": [
        "#funcion fit y loop de entrenamiento\n",
        "def fit(theta, x, y, learning_rate=0.001, epochs=1000):\n",
        "    TOLERANCIA = 0.0001\n",
        "    eps = 1e-3\n",
        "    loss_accum = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = 0.0  # Pérdida acumulada en el epoch actual\n",
        "\n",
        "        for i in range(len(x)):\n",
        "            # Obtener el vector x y la salida esperada y correspondientes\n",
        "            x_i = x[i]\n",
        "            y_i = y[i]\n",
        "\n",
        "            # Calcular el gradiente numérico para el ejemplo actual\n",
        "            gradient = numerical_gradient(theta, x_i, y_i, eps)\n",
        "\n",
        "            # Actualizar los parámetros\n",
        "            theta -= learning_rate * gradient\n",
        "\n",
        "            # Calcular la pérdida para el ejemplo actual\n",
        "            loss_i = funcion_objetivo(theta, x_i, y_i)\n",
        "            loss_epoch += loss_i\n",
        "\n",
        "        # Calcular la pérdida promedio para el epoch actual\n",
        "        loss_avg = loss_epoch / len(x)\n",
        "        loss_accum.append(loss_avg)\n",
        "\n",
        "        if abs(theta - loss_avg) < TOLERANCIA:\n",
        "            break\n",
        "\n",
        "    return loss_accum\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS4sKd_Hvhgo"
      },
      "outputs": [],
      "source": [
        "def predict(x):\n",
        "        y = np.dot(x, theta)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar la red neuronal\n",
        "loss_accum = fit(theta, x, y, learning_rate=0.001, epochs=1000)\n",
        "\n",
        "# Obtener predicciones en el conjunto de datos de prueba\n",
        "y_pred = predict(x)\n",
        "\n",
        "# Calcular el error cuadrático medio en el conjunto de datos de prueba\n",
        "mse = ((y_pred - y) ** 2).mean()\n",
        "\n",
        "# Graficar la función objetivo a lo largo del entrenamiento\n",
        "plt.plot(loss_accum)\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Función Objetivo')\n",
        "plt.title('Evolución de la función objetivo durante el entrenamiento')\n",
        "plt.show()\n",
        "\n",
        "# Graficar el error cuadrático medio en el conjunto de datos de prueba\n",
        "plt.scatter(y, y_pred)\n",
        "plt.xlabel('Valor Real')\n",
        "plt.ylabel('Predicción')\n",
        "plt.title('Predicciones vs. Valores Reales en el conjunto de datos de prueba')\n",
        "plt.show()\n",
        "\n",
        "print(f'Error cuadrático medio en el conjunto de datos de prueba: {mse}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
